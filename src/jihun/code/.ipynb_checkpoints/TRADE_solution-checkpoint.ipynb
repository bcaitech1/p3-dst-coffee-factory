{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ITO1erJr04W8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from data_utils import (\n",
    "    load_dataset, \n",
    "    get_examples_from_dialogues, \n",
    "    convert_state_dict, \n",
    "    DSTInputExample, \n",
    "    OpenVocabDSTFeature, \n",
    "    DSTPreprocessor, \n",
    "    WOSDataset)\n",
    "    \n",
    "from inference import inference\n",
    "from evaluation import _evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1Tw9bZT04XC"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O-6Xn5G504XD",
    "outputId": "b2609353-2c5f-4976-ad3b-aa094984ef3c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6301/6301 [00:00<00:00, 8913.08it/s]\n",
      "100%|██████████| 699/699 [00:00<00:00, 14952.31it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_file = \"/opt/ml/input/data/train_dataset/train_dials.json\"\n",
    "slot_meta = json.load(open(\"/opt/ml/input/data/train_dataset/slot_meta.json\"))\n",
    "ontology = json.load(open(\"/opt/ml/input/data/train_dataset/ontology.json\"))\n",
    "train_data, dev_data, dev_labels = load_dataset(train_data_file)\n",
    "\n",
    "train_examples = get_examples_from_dialogues(train_data,\n",
    "                                             user_first=False,\n",
    "                                             dialogue_level=False)\n",
    "dev_examples = get_examples_from_dialogues(dev_data,\n",
    "                                           user_first=False,\n",
    "                                           dialogue_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6C9sqEXz04XE",
    "outputId": "ad4a4fc8-0f73-468a-ff38-b465afa644b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46088\n",
      "5157\n"
     ]
    }
   ],
   "source": [
    "print(len(train_examples))\n",
    "print(len(dev_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk2bVKMH04XE"
   },
   "source": [
    "## TRADE Preprocessor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaR5NuDp04XE"
   },
   "source": [
    "기존의 GRU 기반의 인코더를 BERT-based Encoder로 바꿀 준비를 합시다.\n",
    "\n",
    "1. 현재 `_convert_example_to_feature`에서는 `max_seq_length`를 핸들하고 있지 않습니다. `input_id`와 `segment_id`가 `max_seq_length`를 넘어가면 좌측부터 truncate시키는 코드를 삽입하세요.\n",
    "\n",
    "2. hybrid approach에서 얻은 교훈을 바탕으로 gate class를 3개에서 5개로 늘려봅시다.\n",
    "    - `gating2id`를 수정하세요\n",
    "    - 이에 따른 `recover_state`를 수정하세요.\n",
    "    \n",
    "3. word dropout을 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Bo89P9d304XF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TRADEPreprocessor(DSTPreprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        slot_meta,\n",
    "        src_tokenizer,\n",
    "        trg_tokenizer=None,\n",
    "        ontology=None,\n",
    "        max_seq_length=512,\n",
    "    ):\n",
    "        self.slot_meta = slot_meta\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer if trg_tokenizer else src_tokenizer\n",
    "        self.ontology = ontology\n",
    "        self.gating2id = {\"none\": 0, \"dontcare\": 1, \"yes\": 2, \"no\": 3, \"ptr\": 4}\n",
    "        self.id2gating = {v: k for k, v in self.gating2id.items()}\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def _convert_example_to_feature(self, example):\n",
    "        dialogue_context = \" [SEP] \".join(example.context_turns + example.current_turn)\n",
    "\n",
    "        input_id = self.src_tokenizer.encode(dialogue_context, add_special_tokens=False)\n",
    "        max_length = self.max_seq_length - 2\n",
    "        if len(input_id) > max_length:\n",
    "            gap = len(input_id) - max_length\n",
    "            input_id = input_id[gap:]\n",
    "\n",
    "        input_id = (\n",
    "            [self.src_tokenizer.cls_token_id]\n",
    "            + input_id\n",
    "            + [self.src_tokenizer.sep_token_id]\n",
    "        )\n",
    "        segment_id = [0] * len(input_id)\n",
    "\n",
    "        target_ids = []\n",
    "        gating_id = []\n",
    "        if not example.label:\n",
    "            example.label = []\n",
    "\n",
    "        state = convert_state_dict(example.label)\n",
    "        for slot in self.slot_meta:\n",
    "            value = state.get(slot, \"none\")\n",
    "            target_id = self.trg_tokenizer.encode(value, add_special_tokens=False) + [\n",
    "                self.trg_tokenizer.sep_token_id\n",
    "            ]\n",
    "            target_ids.append(target_id)\n",
    "            gating_id.append(self.gating2id.get(value, self.gating2id[\"ptr\"]))\n",
    "        target_ids = self.pad_ids(target_ids, self.trg_tokenizer.pad_token_id)\n",
    "        return OpenVocabDSTFeature(\n",
    "            example.guid, input_id, segment_id, gating_id, target_ids\n",
    "        )\n",
    "\n",
    "    def convert_examples_to_features(self, examples):\n",
    "        return list(map(self._convert_example_to_feature, examples))\n",
    "\n",
    "    def recover_state(self, gate_list, gen_list):\n",
    "        assert len(gate_list) == len(self.slot_meta)\n",
    "        assert len(gen_list) == len(self.slot_meta)\n",
    "\n",
    "        recovered = []\n",
    "        for slot, gate, value in zip(self.slot_meta, gate_list, gen_list):\n",
    "            if self.id2gating[gate] == \"none\":\n",
    "                continue\n",
    "\n",
    "            if self.id2gating[gate] in [\"dontcare\", \"yes\", \"no\"]:\n",
    "                recovered.append(\"%s-%s\" % (slot, self.id2gating[gate]))\n",
    "                continue\n",
    "\n",
    "            token_id_list = []\n",
    "            for id_ in value:\n",
    "                if id_ in self.trg_tokenizer.all_special_ids:\n",
    "                    break\n",
    "\n",
    "                token_id_list.append(id_)\n",
    "            value = self.trg_tokenizer.decode(token_id_list, skip_special_tokens=True)\n",
    "\n",
    "            if value == \"none\":\n",
    "                continue\n",
    "\n",
    "            recovered.append(\"%s-%s\" % (slot, value))\n",
    "        return recovered\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        guids = [b.guid for b in batch]\n",
    "        input_ids = torch.LongTensor(\n",
    "            self.pad_ids([b.input_id for b in batch], self.src_tokenizer.pad_token_id)\n",
    "        )\n",
    "        segment_ids = torch.LongTensor(\n",
    "            self.pad_ids([b.segment_id for b in batch], self.src_tokenizer.pad_token_id)\n",
    "        )\n",
    "        input_masks = input_ids.ne(self.src_tokenizer.pad_token_id)\n",
    "        \n",
    "        gating_ids = torch.LongTensor([b.gating_id for b in batch])\n",
    "        target_ids = self.pad_id_of_matrix(\n",
    "            [torch.LongTensor(b.target_ids) for b in batch],\n",
    "            self.trg_tokenizer.pad_token_id,\n",
    "        )\n",
    "        return input_ids, segment_ids, input_masks, gating_ids, target_ids, guids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd47XExT04XF"
   },
   "source": [
    "## Convert_Examples_to_Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zOjho3J404XG",
    "outputId": "682af29a-6698-469b-b866-2caccfbaf555",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "processor = TRADEPreprocessor(slot_meta, tokenizer, max_seq_length=512)\n",
    "\n",
    "train_features = processor.convert_examples_to_features(train_examples)\n",
    "dev_features = processor.convert_examples_to_features(dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DFNdbCsB04XG",
    "outputId": "fb7b0fa6-3d1c-4a19-9a33-53e46e0a88b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46088\n",
      "5157\n"
     ]
    }
   ],
   "source": [
    "print(len(train_features))\n",
    "print(len(dev_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovQOZ9Sz04XH"
   },
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaivCGee04XH"
   },
   "source": [
    "1. `GRUEncoder`를 `BertModel`로 교체하세요. 이에 따라 `tie_weight` 함수가 수정되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "A7M4oZ1w04XH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TRADE(nn.Module):\n",
    "    def __init__(self, config, slot_vocab, slot_meta, pad_idx=0):\n",
    "        super(TRADE, self).__init__()\n",
    "        self.slot_meta = slot_meta\n",
    "        if config.model_name_or_path:\n",
    "            self.encoder = BertModel.from_pretrained(config.model_name_or_path)\n",
    "        else:\n",
    "            self.encoder = BertModel(config)\n",
    "            \n",
    "        self.decoder = SlotGenerator(\n",
    "            config.vocab_size,\n",
    "            config.hidden_size,\n",
    "            config.hidden_dropout_prob,\n",
    "            config.n_gate,\n",
    "            None,\n",
    "            pad_idx,\n",
    "        )\n",
    "        \n",
    "        # init for only subword embedding\n",
    "        self.decoder.set_slot_idx(slot_vocab)\n",
    "        self.tie_weight()\n",
    "\n",
    "    def tie_weight(self):\n",
    "        self.decoder.embed.weight = self.encoder.embeddings.word_embeddings.weight\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask=None, max_len=10, teacher=None):\n",
    "\n",
    "        encoder_outputs, pooled_output = self.encoder(input_ids=input_ids)\n",
    "        all_point_outputs, all_gate_outputs = self.decoder(\n",
    "            input_ids, encoder_outputs, pooled_output.unsqueeze(0), attention_mask, max_len, teacher\n",
    "        )\n",
    "\n",
    "        return all_point_outputs, all_gate_outputs\n",
    "    \n",
    "class SlotGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, hidden_size, dropout, n_gate, proj_dim=None, pad_idx=0\n",
    "    ):\n",
    "        super(SlotGenerator, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, hidden_size, padding_idx=pad_idx\n",
    "        )  # shared with encoder\n",
    "\n",
    "        if proj_dim:\n",
    "            self.proj_layer = nn.Linear(hidden_size, proj_dim, bias=False)\n",
    "        else:\n",
    "            self.proj_layer = None\n",
    "        self.hidden_size = proj_dim if proj_dim else hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            self.hidden_size, self.hidden_size, 1, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.n_gate = n_gate\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_gen = nn.Linear(self.hidden_size * 3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.w_gate = nn.Linear(self.hidden_size, n_gate)\n",
    "\n",
    "    def set_slot_idx(self, slot_vocab_idx):\n",
    "        whole = []\n",
    "        max_length = max(map(len, slot_vocab_idx))\n",
    "        for idx in slot_vocab_idx:\n",
    "            if len(idx) < max_length:\n",
    "                gap = max_length - len(idx)\n",
    "                idx.extend([self.pad_idx] * gap)\n",
    "            whole.append(idx)\n",
    "        self.slot_embed_idx = whole  # torch.LongTensor(whole)\n",
    "\n",
    "    def embedding(self, x):\n",
    "        x = self.embed(x)\n",
    "        if self.proj_layer:\n",
    "            x = self.proj_layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, encoder_output, hidden, input_masks, max_len, teacher=None\n",
    "    ):\n",
    "        input_masks = input_masks.ne(1)\n",
    "        # J, slot_meta : key : [domain, slot] ex> LongTensor([1,2])\n",
    "        # J,2\n",
    "        batch_size = encoder_output.size(0)\n",
    "        slot = torch.LongTensor(self.slot_embed_idx).to(input_ids.device)  ##\n",
    "        slot_e = torch.sum(self.embedding(slot), 1)  # J,d\n",
    "        J = slot_e.size(0)\n",
    "\n",
    "        all_point_outputs = torch.zeros(batch_size, J, max_len, self.vocab_size).to(\n",
    "            input_ids.device\n",
    "        )\n",
    "        \n",
    "        # Parallel Decoding\n",
    "        w = slot_e.repeat(batch_size, 1).unsqueeze(1)\n",
    "        hidden = hidden.repeat_interleave(J, dim=1)\n",
    "        encoder_output = encoder_output.repeat_interleave(J, dim=0)\n",
    "        input_ids = input_ids.repeat_interleave(J, dim=0)\n",
    "        input_masks = input_masks.repeat_interleave(J, dim=0)\n",
    "        for k in range(max_len):\n",
    "            w = self.dropout(w)\n",
    "            _, hidden = self.gru(w, hidden)  # 1,B,D\n",
    "\n",
    "            # B,T,D * B,D,1 => B,T\n",
    "            attn_e = torch.bmm(encoder_output, hidden.permute(1, 2, 0))  # B,T,1\n",
    "            attn_e = attn_e.squeeze(-1).masked_fill(input_masks, -1e9)\n",
    "            attn_history = F.softmax(attn_e, -1)  # B,T\n",
    "\n",
    "            if self.proj_layer:\n",
    "                hidden_proj = torch.matmul(hidden, self.proj_layer.weight)\n",
    "            else:\n",
    "                hidden_proj = hidden\n",
    "\n",
    "            # B,D * D,V => B,V\n",
    "            attn_v = torch.matmul(\n",
    "                hidden_proj.squeeze(0), self.embed.weight.transpose(0, 1)\n",
    "            )  # B,V\n",
    "            attn_vocab = F.softmax(attn_v, -1)\n",
    "\n",
    "            # B,1,T * B,T,D => B,1,D\n",
    "            context = torch.bmm(attn_history.unsqueeze(1), encoder_output)  # B,1,D\n",
    "            p_gen = self.sigmoid(\n",
    "                self.w_gen(torch.cat([w, hidden.transpose(0, 1), context], -1))\n",
    "            )  # B,1\n",
    "            p_gen = p_gen.squeeze(-1)\n",
    "\n",
    "            p_context_ptr = torch.zeros_like(attn_vocab).to(input_ids.device)\n",
    "            p_context_ptr.scatter_add_(1, input_ids, attn_history)  # copy B,V\n",
    "            p_final = p_gen * attn_vocab + (1 - p_gen) * p_context_ptr  # B,V\n",
    "            _, w_idx = p_final.max(-1)\n",
    "\n",
    "            if teacher is not None:\n",
    "                w = self.embedding(teacher[:, :, k]).transpose(0, 1).reshape(batch_size * J, 1, -1)\n",
    "            else:\n",
    "                w = self.embedding(w_idx).unsqueeze(1)  # B,1,D\n",
    "            if k == 0:\n",
    "                gated_logit = self.w_gate(context.squeeze(1))  # B,3\n",
    "                all_gate_outputs = gated_logit.view(batch_size, J, self.n_gate)\n",
    "            all_point_outputs[:, :, k, :] = p_final.view(batch_size, J, self.vocab_size)\n",
    "\n",
    "        return all_point_outputs, all_gate_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdLS3jkA04XI"
   },
   "source": [
    "# 모델 및 데이터 로더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-MYTPvu004XI",
    "outputId": "4631f60a-11b1-450e-a5a8-8abbcaa3f225"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "slot_vocab = []\n",
    "for slot in slot_meta:\n",
    "    slot_vocab.append(\n",
    "        tokenizer.encode(slot.replace('-', ' '),\n",
    "                         add_special_tokens=False)\n",
    "    )\n",
    "    \n",
    "config = BertConfig.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "config.model_name_or_path = 'monologg/koelectra-base-v3-discriminator'\n",
    "config.n_gate = len(processor.gating2id)\n",
    "config.proj_dim = None\n",
    "model = TRADE(config, slot_vocab, slot_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ksvFxR-h04XI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = WOSDataset(train_features)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, batch_size=16, sampler=train_sampler, collate_fn=processor.collate_fn)\n",
    "\n",
    "dev_data = WOSDataset(dev_features)\n",
    "dev_sampler = SequentialSampler(dev_data)\n",
    "dev_loader = DataLoader(dev_data, batch_size=8, sampler=dev_sampler, collate_fn=processor.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVYxBaXE04XJ"
   },
   "source": [
    "# Optimizer & Scheduler 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NmDmrigU04XJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "t_total = len(train_loader) * n_epochs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0.1, num_training_steps=t_total\n",
    ")\n",
    "teacher_forcing = 0.5\n",
    "model.to(device)\n",
    "\n",
    "def masked_cross_entropy_for_value(logits, target, pad_idx=0):\n",
    "    mask = target.ne(pad_idx)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    log_probs_flat = torch.log(logits_flat)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / (mask.sum().float())\n",
    "    return loss\n",
    "\n",
    "loss_fnc_1 = masked_cross_entropy_for_value  # generation\n",
    "loss_fnc_2 = nn.CrossEntropyLoss()  # gating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0GK3Z8z04XJ"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrWnCWwC04XJ",
    "outputId": "194c265e-ab26-4993-a61e-1dca5d3274aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10] [0/2881] 10.893712\n",
      "[0/10] [100/2881] 1.742175\n",
      "[0/10] [200/2881] 1.795856\n",
      "[0/10] [300/2881] 1.353076\n",
      "[0/10] [400/2881] 1.139327\n",
      "[0/10] [500/2881] 1.060887\n",
      "[0/10] [600/2881] 0.968178\n"
     ]
    }
   ],
   "source": [
    " for epoch in range(n_epochs):\n",
    "    batch_loss = []\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids, segment_ids, input_masks, gating_ids, target_ids, guids = [b.to(device) if not isinstance(b, list) else b for b in batch]\n",
    "        if teacher_forcing > 0.0 and random.random() < teacher_forcing:\n",
    "            tf = target_ids\n",
    "        else:\n",
    "            tf = None\n",
    "\n",
    "        all_point_outputs, all_gate_outputs = model(input_ids, segment_ids, input_masks, target_ids.size(-1))  # gt - length (generation)\n",
    "        loss_1 = loss_fnc_1(all_point_outputs.contiguous(), target_ids.contiguous().view(-1))\n",
    "        loss_2 = loss_fnc_2(all_gate_outputs.contiguous().view(-1, 5), gating_ids.contiguous().view(-1))\n",
    "        loss = loss_1 + loss_2\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step % 100 == 0:\n",
    "            print('[%d/%d] [%d/%d] %f' % (epoch, n_epochs, step, len(train_loader), loss.item()))\n",
    "                \n",
    "    predictions = inference(model, dev_loader, processor, device)\n",
    "    eval_result = _evaluation(predictions, dev_labels, slot_meta)\n",
    "    for k, v in eval_result.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR4EEzbz04XK"
   },
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOJqYcpP04XK",
    "outputId": "8947b1d7-fcdc-4c4b-8536-a8ee7a057123"
   },
   "outputs": [],
   "source": [
    "eval_data = json.load(open(f\"/opt/ml/input/data/eval_dataset/eval_dials.json\", \"r\"))\n",
    "\n",
    "eval_examples = get_examples_from_dialogues(\n",
    "    eval_data, user_first=False, dialogue_level=False\n",
    ")\n",
    "\n",
    "# Extracting Featrues\n",
    "eval_features = processor.convert_examples_to_features(eval_examples)\n",
    "eval_data = WOSDataset(eval_features)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_loader = DataLoader(\n",
    "    eval_data,\n",
    "    batch_size=8,\n",
    "    sampler=eval_sampler,\n",
    "    collate_fn=processor.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLU16_Ur04XL",
    "outputId": "c30e4ae5-5212-41f8-f171-0ff5e56b02fb"
   },
   "outputs": [],
   "source": [
    "predictions = inference(model, eval_loader, processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0uR9J8e04XL"
   },
   "outputs": [],
   "source": [
    "json.dump(predictions, open('predictions.csv', 'w'), indent=2, ensure_ascii=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhrO4LRQ04XL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TRADE_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
