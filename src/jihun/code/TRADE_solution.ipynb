{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ITO1erJr04W8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from data_utils import (\n",
    "    load_dataset, \n",
    "    get_examples_from_dialogues, \n",
    "    convert_state_dict, \n",
    "    DSTInputExample, \n",
    "    OpenVocabDSTFeature, \n",
    "    DSTPreprocessor, \n",
    "    WOSDataset)\n",
    "    \n",
    "from inference import inference\n",
    "from evaluation import _evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1Tw9bZT04XC"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O-6Xn5G504XD",
    "outputId": "b2609353-2c5f-4976-ad3b-aa094984ef3c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6301/6301 [00:00<00:00, 8931.42it/s]\n",
      "100%|██████████| 699/699 [00:00<00:00, 15593.78it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_file = \"/opt/ml/input/data/train_dataset/train_dials.json\"\n",
    "slot_meta = json.load(open(\"/opt/ml/input/data/train_dataset/slot_meta.json\"))\n",
    "ontology = json.load(open(\"/opt/ml/input/data/train_dataset/ontology.json\"))\n",
    "train_data, dev_data, dev_labels = load_dataset(train_data_file)\n",
    "\n",
    "train_examples = get_examples_from_dialogues(train_data,\n",
    "                                             user_first=False,\n",
    "                                             dialogue_level=False)\n",
    "dev_examples = get_examples_from_dialogues(dev_data,\n",
    "                                           user_first=False,\n",
    "                                           dialogue_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6C9sqEXz04XE",
    "outputId": "ad4a4fc8-0f73-468a-ff38-b465afa644b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46182\n",
      "5063\n"
     ]
    }
   ],
   "source": [
    "print(len(train_examples))\n",
    "print(len(dev_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk2bVKMH04XE"
   },
   "source": [
    "## TRADE Preprocessor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaR5NuDp04XE"
   },
   "source": [
    "기존의 GRU 기반의 인코더를 BERT-based Encoder로 바꿀 준비를 합시다.\n",
    "\n",
    "1. 현재 `_convert_example_to_feature`에서는 `max_seq_length`를 핸들하고 있지 않습니다. `input_id`와 `segment_id`가 `max_seq_length`를 넘어가면 좌측부터 truncate시키는 코드를 삽입하세요.\n",
    "\n",
    "2. hybrid approach에서 얻은 교훈을 바탕으로 gate class를 3개에서 5개로 늘려봅시다.\n",
    "    - `gating2id`를 수정하세요\n",
    "    - 이에 따른 `recover_state`를 수정하세요.\n",
    "    \n",
    "3. word dropout을 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Bo89P9d304XF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TRADEPreprocessor(DSTPreprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        slot_meta,\n",
    "        src_tokenizer,\n",
    "        trg_tokenizer=None,\n",
    "        ontology=None,\n",
    "        max_seq_length=512,\n",
    "    ):\n",
    "        self.slot_meta = slot_meta\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer if trg_tokenizer else src_tokenizer\n",
    "        self.ontology = ontology\n",
    "        self.gating2id = {\"none\": 0, \"dontcare\": 1, \"yes\": 2, \"no\": 3, \"ptr\": 4}\n",
    "        self.id2gating = {v: k for k, v in self.gating2id.items()}\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def _convert_example_to_feature(self, example):\n",
    "        dialogue_context = \" [SEP] \".join(example.context_turns + example.current_turn)\n",
    "\n",
    "        input_id = self.src_tokenizer.encode(dialogue_context, add_special_tokens=False)\n",
    "        max_length = self.max_seq_length - 2\n",
    "        if len(input_id) > max_length:\n",
    "            gap = len(input_id) - max_length\n",
    "            input_id = input_id[gap:]\n",
    "\n",
    "        input_id = (\n",
    "            [self.src_tokenizer.cls_token_id]\n",
    "            + input_id\n",
    "            + [self.src_tokenizer.sep_token_id]\n",
    "        )\n",
    "        segment_id = [0] * len(input_id)\n",
    "\n",
    "        target_ids = []\n",
    "        gating_id = []\n",
    "        if not example.label:\n",
    "            example.label = []\n",
    "\n",
    "        state = convert_state_dict(example.label)\n",
    "        for slot in self.slot_meta:\n",
    "            value = state.get(slot, \"none\")\n",
    "            target_id = self.trg_tokenizer.encode(value, add_special_tokens=False) + [\n",
    "                self.trg_tokenizer.sep_token_id\n",
    "            ]\n",
    "            target_ids.append(target_id)\n",
    "            gating_id.append(self.gating2id.get(value, self.gating2id[\"ptr\"]))\n",
    "        target_ids = self.pad_ids(target_ids, self.trg_tokenizer.pad_token_id)\n",
    "        return OpenVocabDSTFeature(\n",
    "            example.guid, input_id, segment_id, gating_id, target_ids\n",
    "        )\n",
    "\n",
    "    def convert_examples_to_features(self, examples):\n",
    "        return list(map(self._convert_example_to_feature, examples))\n",
    "\n",
    "    def recover_state(self, gate_list, gen_list):\n",
    "        assert len(gate_list) == len(self.slot_meta)\n",
    "        assert len(gen_list) == len(self.slot_meta)\n",
    "\n",
    "        recovered = []\n",
    "        for slot, gate, value in zip(self.slot_meta, gate_list, gen_list):\n",
    "            if self.id2gating[gate] == \"none\":\n",
    "                continue\n",
    "\n",
    "            if self.id2gating[gate] in [\"dontcare\", \"yes\", \"no\"]:\n",
    "                recovered.append(\"%s-%s\" % (slot, self.id2gating[gate]))\n",
    "                continue\n",
    "\n",
    "            token_id_list = []\n",
    "            for id_ in value:\n",
    "                if id_ in self.trg_tokenizer.all_special_ids:\n",
    "                    break\n",
    "\n",
    "                token_id_list.append(id_)\n",
    "            value = self.trg_tokenizer.decode(token_id_list, skip_special_tokens=True)\n",
    "\n",
    "            if value == \"none\":\n",
    "                continue\n",
    "\n",
    "            recovered.append(\"%s-%s\" % (slot, value))\n",
    "        return recovered\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        guids = [b.guid for b in batch]\n",
    "        input_ids = torch.LongTensor(\n",
    "            self.pad_ids([b.input_id for b in batch], self.src_tokenizer.pad_token_id)\n",
    "        )\n",
    "        segment_ids = torch.LongTensor(\n",
    "            self.pad_ids([b.segment_id for b in batch], self.src_tokenizer.pad_token_id)\n",
    "        )\n",
    "        input_masks = input_ids.ne(self.src_tokenizer.pad_token_id)\n",
    "        \n",
    "        gating_ids = torch.LongTensor([b.gating_id for b in batch])\n",
    "        target_ids = self.pad_id_of_matrix(\n",
    "            [torch.LongTensor(b.target_ids) for b in batch],\n",
    "            self.trg_tokenizer.pad_token_id,\n",
    "        )\n",
    "        return input_ids, segment_ids, input_masks, gating_ids, target_ids, guids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd47XExT04XF"
   },
   "source": [
    "## Convert_Examples_to_Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zOjho3J404XG",
    "outputId": "682af29a-6698-469b-b866-2caccfbaf555",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "processor = TRADEPreprocessor(slot_meta, tokenizer, max_seq_length=512)\n",
    "\n",
    "train_features = processor.convert_examples_to_features(train_examples)\n",
    "dev_features = processor.convert_examples_to_features(dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DFNdbCsB04XG",
    "outputId": "fb7b0fa6-3d1c-4a19-9a33-53e46e0a88b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46182\n",
      "5063\n"
     ]
    }
   ],
   "source": [
    "print(len(train_features))\n",
    "print(len(dev_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovQOZ9Sz04XH"
   },
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaivCGee04XH"
   },
   "source": [
    "1. `GRUEncoder`를 `BertModel`로 교체하세요. 이에 따라 `tie_weight` 함수가 수정되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "A7M4oZ1w04XH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TRADE(nn.Module):\n",
    "    def __init__(self, config, slot_vocab, slot_meta, pad_idx=0):\n",
    "        super(TRADE, self).__init__()\n",
    "        self.slot_meta = slot_meta\n",
    "        if config.model_name_or_path:\n",
    "            self.encoder = BertModel.from_pretrained(config.model_name_or_path)\n",
    "        else:\n",
    "            self.encoder = BertModel(config)\n",
    "            \n",
    "        self.decoder = SlotGenerator(\n",
    "            config.vocab_size,\n",
    "            config.hidden_size,\n",
    "            config.hidden_dropout_prob,\n",
    "            config.n_gate,\n",
    "            None,\n",
    "            pad_idx,\n",
    "        )\n",
    "        \n",
    "        # init for only subword embedding\n",
    "        self.decoder.set_slot_idx(slot_vocab)\n",
    "        self.tie_weight()\n",
    "\n",
    "    def tie_weight(self):\n",
    "        self.decoder.embed.weight = self.encoder.embeddings.word_embeddings.weight\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask=None, max_len=10, teacher=None):\n",
    "\n",
    "        encoder_outputs, pooled_output = self.encoder(input_ids=input_ids)\n",
    "        all_point_outputs, all_gate_outputs = self.decoder(\n",
    "            input_ids, encoder_outputs, pooled_output.unsqueeze(0), attention_mask, max_len, teacher\n",
    "        )\n",
    "\n",
    "        return all_point_outputs, all_gate_outputs\n",
    "    \n",
    "class SlotGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, hidden_size, dropout, n_gate, proj_dim=None, pad_idx=0\n",
    "    ):\n",
    "        super(SlotGenerator, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, hidden_size, padding_idx=pad_idx\n",
    "        )  # shared with encoder\n",
    "\n",
    "        if proj_dim:\n",
    "            self.proj_layer = nn.Linear(hidden_size, proj_dim, bias=False)\n",
    "        else:\n",
    "            self.proj_layer = None\n",
    "        self.hidden_size = proj_dim if proj_dim else hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            self.hidden_size, self.hidden_size, 1, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.n_gate = n_gate\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_gen = nn.Linear(self.hidden_size * 3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.w_gate = nn.Linear(self.hidden_size, n_gate)\n",
    "\n",
    "    def set_slot_idx(self, slot_vocab_idx):\n",
    "        whole = []\n",
    "        max_length = max(map(len, slot_vocab_idx))\n",
    "        for idx in slot_vocab_idx:\n",
    "            if len(idx) < max_length:\n",
    "                gap = max_length - len(idx)\n",
    "                idx.extend([self.pad_idx] * gap)\n",
    "            whole.append(idx)\n",
    "        self.slot_embed_idx = whole  # torch.LongTensor(whole)\n",
    "\n",
    "    def embedding(self, x):\n",
    "        x = self.embed(x)\n",
    "        if self.proj_layer:\n",
    "            x = self.proj_layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, encoder_output, hidden, input_masks, max_len, teacher=None\n",
    "    ):\n",
    "        input_masks = input_masks.ne(1)\n",
    "        # J, slot_meta : key : [domain, slot] ex> LongTensor([1,2])\n",
    "        # J,2\n",
    "        batch_size = encoder_output.size(0)\n",
    "        slot = torch.LongTensor(self.slot_embed_idx).to(input_ids.device)  ##\n",
    "        slot_e = torch.sum(self.embedding(slot), 1)  # J,d\n",
    "        J = slot_e.size(0)\n",
    "\n",
    "        all_point_outputs = torch.zeros(batch_size, J, max_len, self.vocab_size).to(\n",
    "            input_ids.device\n",
    "        )\n",
    "        \n",
    "        # Parallel Decoding\n",
    "        w = slot_e.repeat(batch_size, 1).unsqueeze(1)\n",
    "        hidden = hidden.repeat_interleave(J, dim=1)\n",
    "        encoder_output = encoder_output.repeat_interleave(J, dim=0)\n",
    "        input_ids = input_ids.repeat_interleave(J, dim=0)\n",
    "        input_masks = input_masks.repeat_interleave(J, dim=0)\n",
    "        for k in range(max_len):\n",
    "            w = self.dropout(w)\n",
    "            _, hidden = self.gru(w, hidden)  # 1,B,D\n",
    "\n",
    "            # B,T,D * B,D,1 => B,T\n",
    "            attn_e = torch.bmm(encoder_output, hidden.permute(1, 2, 0))  # B,T,1\n",
    "            attn_e = attn_e.squeeze(-1).masked_fill(input_masks, -1e9)\n",
    "            attn_history = F.softmax(attn_e, -1)  # B,T\n",
    "\n",
    "            if self.proj_layer:\n",
    "                hidden_proj = torch.matmul(hidden, self.proj_layer.weight)\n",
    "            else:\n",
    "                hidden_proj = hidden\n",
    "\n",
    "            # B,D * D,V => B,V\n",
    "            attn_v = torch.matmul(\n",
    "                hidden_proj.squeeze(0), self.embed.weight.transpose(0, 1)\n",
    "            )  # B,V\n",
    "            attn_vocab = F.softmax(attn_v, -1)\n",
    "\n",
    "            # B,1,T * B,T,D => B,1,D\n",
    "            context = torch.bmm(attn_history.unsqueeze(1), encoder_output)  # B,1,D\n",
    "            p_gen = self.sigmoid(\n",
    "                self.w_gen(torch.cat([w, hidden.transpose(0, 1), context], -1))\n",
    "            )  # B,1\n",
    "            p_gen = p_gen.squeeze(-1)\n",
    "\n",
    "            p_context_ptr = torch.zeros_like(attn_vocab).to(input_ids.device)\n",
    "            p_context_ptr.scatter_add_(1, input_ids, attn_history)  # copy B,V\n",
    "            p_final = p_gen * attn_vocab + (1 - p_gen) * p_context_ptr  # B,V\n",
    "            _, w_idx = p_final.max(-1)\n",
    "\n",
    "            if teacher is not None:\n",
    "                w = self.embedding(teacher[:, :, k]).transpose(0, 1).reshape(batch_size * J, 1, -1)\n",
    "            else:\n",
    "                w = self.embedding(w_idx).unsqueeze(1)  # B,1,D\n",
    "            if k == 0:\n",
    "                gated_logit = self.w_gate(context.squeeze(1))  # B,3\n",
    "                all_gate_outputs = gated_logit.view(batch_size, J, self.n_gate)\n",
    "            all_point_outputs[:, :, k, :] = p_final.view(batch_size, J, self.vocab_size)\n",
    "\n",
    "        return all_point_outputs, all_gate_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdLS3jkA04XI"
   },
   "source": [
    "# 모델 및 데이터 로더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-MYTPvu004XI",
    "outputId": "4631f60a-11b1-450e-a5a8-8abbcaa3f225"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing BertModel: ['electra.embeddings.position_ids', 'electra.embeddings.word_embeddings.weight', 'electra.embeddings.position_embeddings.weight', 'electra.embeddings.token_type_embeddings.weight', 'electra.embeddings.LayerNorm.weight', 'electra.embeddings.LayerNorm.bias', 'electra.encoder.layer.0.attention.self.query.weight', 'electra.encoder.layer.0.attention.self.query.bias', 'electra.encoder.layer.0.attention.self.key.weight', 'electra.encoder.layer.0.attention.self.key.bias', 'electra.encoder.layer.0.attention.self.value.weight', 'electra.encoder.layer.0.attention.self.value.bias', 'electra.encoder.layer.0.attention.output.dense.weight', 'electra.encoder.layer.0.attention.output.dense.bias', 'electra.encoder.layer.0.attention.output.LayerNorm.weight', 'electra.encoder.layer.0.attention.output.LayerNorm.bias', 'electra.encoder.layer.0.intermediate.dense.weight', 'electra.encoder.layer.0.intermediate.dense.bias', 'electra.encoder.layer.0.output.dense.weight', 'electra.encoder.layer.0.output.dense.bias', 'electra.encoder.layer.0.output.LayerNorm.weight', 'electra.encoder.layer.0.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.self.query.weight', 'electra.encoder.layer.1.attention.self.query.bias', 'electra.encoder.layer.1.attention.self.key.weight', 'electra.encoder.layer.1.attention.self.key.bias', 'electra.encoder.layer.1.attention.self.value.weight', 'electra.encoder.layer.1.attention.self.value.bias', 'electra.encoder.layer.1.attention.output.dense.weight', 'electra.encoder.layer.1.attention.output.dense.bias', 'electra.encoder.layer.1.attention.output.LayerNorm.weight', 'electra.encoder.layer.1.attention.output.LayerNorm.bias', 'electra.encoder.layer.1.intermediate.dense.weight', 'electra.encoder.layer.1.intermediate.dense.bias', 'electra.encoder.layer.1.output.dense.weight', 'electra.encoder.layer.1.output.dense.bias', 'electra.encoder.layer.1.output.LayerNorm.weight', 'electra.encoder.layer.1.output.LayerNorm.bias', 'electra.encoder.layer.2.attention.self.query.weight', 'electra.encoder.layer.2.attention.self.query.bias', 'electra.encoder.layer.2.attention.self.key.weight', 'electra.encoder.layer.2.attention.self.key.bias', 'electra.encoder.layer.2.attention.self.value.weight', 'electra.encoder.layer.2.attention.self.value.bias', 'electra.encoder.layer.2.attention.output.dense.weight', 'electra.encoder.layer.2.attention.output.dense.bias', 'electra.encoder.layer.2.attention.output.LayerNorm.weight', 'electra.encoder.layer.2.attention.output.LayerNorm.bias', 'electra.encoder.layer.2.intermediate.dense.weight', 'electra.encoder.layer.2.intermediate.dense.bias', 'electra.encoder.layer.2.output.dense.weight', 'electra.encoder.layer.2.output.dense.bias', 'electra.encoder.layer.2.output.LayerNorm.weight', 'electra.encoder.layer.2.output.LayerNorm.bias', 'electra.encoder.layer.3.attention.self.query.weight', 'electra.encoder.layer.3.attention.self.query.bias', 'electra.encoder.layer.3.attention.self.key.weight', 'electra.encoder.layer.3.attention.self.key.bias', 'electra.encoder.layer.3.attention.self.value.weight', 'electra.encoder.layer.3.attention.self.value.bias', 'electra.encoder.layer.3.attention.output.dense.weight', 'electra.encoder.layer.3.attention.output.dense.bias', 'electra.encoder.layer.3.attention.output.LayerNorm.weight', 'electra.encoder.layer.3.attention.output.LayerNorm.bias', 'electra.encoder.layer.3.intermediate.dense.weight', 'electra.encoder.layer.3.intermediate.dense.bias', 'electra.encoder.layer.3.output.dense.weight', 'electra.encoder.layer.3.output.dense.bias', 'electra.encoder.layer.3.output.LayerNorm.weight', 'electra.encoder.layer.3.output.LayerNorm.bias', 'electra.encoder.layer.4.attention.self.query.weight', 'electra.encoder.layer.4.attention.self.query.bias', 'electra.encoder.layer.4.attention.self.key.weight', 'electra.encoder.layer.4.attention.self.key.bias', 'electra.encoder.layer.4.attention.self.value.weight', 'electra.encoder.layer.4.attention.self.value.bias', 'electra.encoder.layer.4.attention.output.dense.weight', 'electra.encoder.layer.4.attention.output.dense.bias', 'electra.encoder.layer.4.attention.output.LayerNorm.weight', 'electra.encoder.layer.4.attention.output.LayerNorm.bias', 'electra.encoder.layer.4.intermediate.dense.weight', 'electra.encoder.layer.4.intermediate.dense.bias', 'electra.encoder.layer.4.output.dense.weight', 'electra.encoder.layer.4.output.dense.bias', 'electra.encoder.layer.4.output.LayerNorm.weight', 'electra.encoder.layer.4.output.LayerNorm.bias', 'electra.encoder.layer.5.attention.self.query.weight', 'electra.encoder.layer.5.attention.self.query.bias', 'electra.encoder.layer.5.attention.self.key.weight', 'electra.encoder.layer.5.attention.self.key.bias', 'electra.encoder.layer.5.attention.self.value.weight', 'electra.encoder.layer.5.attention.self.value.bias', 'electra.encoder.layer.5.attention.output.dense.weight', 'electra.encoder.layer.5.attention.output.dense.bias', 'electra.encoder.layer.5.attention.output.LayerNorm.weight', 'electra.encoder.layer.5.attention.output.LayerNorm.bias', 'electra.encoder.layer.5.intermediate.dense.weight', 'electra.encoder.layer.5.intermediate.dense.bias', 'electra.encoder.layer.5.output.dense.weight', 'electra.encoder.layer.5.output.dense.bias', 'electra.encoder.layer.5.output.LayerNorm.weight', 'electra.encoder.layer.5.output.LayerNorm.bias', 'electra.encoder.layer.6.attention.self.query.weight', 'electra.encoder.layer.6.attention.self.query.bias', 'electra.encoder.layer.6.attention.self.key.weight', 'electra.encoder.layer.6.attention.self.key.bias', 'electra.encoder.layer.6.attention.self.value.weight', 'electra.encoder.layer.6.attention.self.value.bias', 'electra.encoder.layer.6.attention.output.dense.weight', 'electra.encoder.layer.6.attention.output.dense.bias', 'electra.encoder.layer.6.attention.output.LayerNorm.weight', 'electra.encoder.layer.6.attention.output.LayerNorm.bias', 'electra.encoder.layer.6.intermediate.dense.weight', 'electra.encoder.layer.6.intermediate.dense.bias', 'electra.encoder.layer.6.output.dense.weight', 'electra.encoder.layer.6.output.dense.bias', 'electra.encoder.layer.6.output.LayerNorm.weight', 'electra.encoder.layer.6.output.LayerNorm.bias', 'electra.encoder.layer.7.attention.self.query.weight', 'electra.encoder.layer.7.attention.self.query.bias', 'electra.encoder.layer.7.attention.self.key.weight', 'electra.encoder.layer.7.attention.self.key.bias', 'electra.encoder.layer.7.attention.self.value.weight', 'electra.encoder.layer.7.attention.self.value.bias', 'electra.encoder.layer.7.attention.output.dense.weight', 'electra.encoder.layer.7.attention.output.dense.bias', 'electra.encoder.layer.7.attention.output.LayerNorm.weight', 'electra.encoder.layer.7.attention.output.LayerNorm.bias', 'electra.encoder.layer.7.intermediate.dense.weight', 'electra.encoder.layer.7.intermediate.dense.bias', 'electra.encoder.layer.7.output.dense.weight', 'electra.encoder.layer.7.output.dense.bias', 'electra.encoder.layer.7.output.LayerNorm.weight', 'electra.encoder.layer.7.output.LayerNorm.bias', 'electra.encoder.layer.8.attention.self.query.weight', 'electra.encoder.layer.8.attention.self.query.bias', 'electra.encoder.layer.8.attention.self.key.weight', 'electra.encoder.layer.8.attention.self.key.bias', 'electra.encoder.layer.8.attention.self.value.weight', 'electra.encoder.layer.8.attention.self.value.bias', 'electra.encoder.layer.8.attention.output.dense.weight', 'electra.encoder.layer.8.attention.output.dense.bias', 'electra.encoder.layer.8.attention.output.LayerNorm.weight', 'electra.encoder.layer.8.attention.output.LayerNorm.bias', 'electra.encoder.layer.8.intermediate.dense.weight', 'electra.encoder.layer.8.intermediate.dense.bias', 'electra.encoder.layer.8.output.dense.weight', 'electra.encoder.layer.8.output.dense.bias', 'electra.encoder.layer.8.output.LayerNorm.weight', 'electra.encoder.layer.8.output.LayerNorm.bias', 'electra.encoder.layer.9.attention.self.query.weight', 'electra.encoder.layer.9.attention.self.query.bias', 'electra.encoder.layer.9.attention.self.key.weight', 'electra.encoder.layer.9.attention.self.key.bias', 'electra.encoder.layer.9.attention.self.value.weight', 'electra.encoder.layer.9.attention.self.value.bias', 'electra.encoder.layer.9.attention.output.dense.weight', 'electra.encoder.layer.9.attention.output.dense.bias', 'electra.encoder.layer.9.attention.output.LayerNorm.weight', 'electra.encoder.layer.9.attention.output.LayerNorm.bias', 'electra.encoder.layer.9.intermediate.dense.weight', 'electra.encoder.layer.9.intermediate.dense.bias', 'electra.encoder.layer.9.output.dense.weight', 'electra.encoder.layer.9.output.dense.bias', 'electra.encoder.layer.9.output.LayerNorm.weight', 'electra.encoder.layer.9.output.LayerNorm.bias', 'electra.encoder.layer.10.attention.self.query.weight', 'electra.encoder.layer.10.attention.self.query.bias', 'electra.encoder.layer.10.attention.self.key.weight', 'electra.encoder.layer.10.attention.self.key.bias', 'electra.encoder.layer.10.attention.self.value.weight', 'electra.encoder.layer.10.attention.self.value.bias', 'electra.encoder.layer.10.attention.output.dense.weight', 'electra.encoder.layer.10.attention.output.dense.bias', 'electra.encoder.layer.10.attention.output.LayerNorm.weight', 'electra.encoder.layer.10.attention.output.LayerNorm.bias', 'electra.encoder.layer.10.intermediate.dense.weight', 'electra.encoder.layer.10.intermediate.dense.bias', 'electra.encoder.layer.10.output.dense.weight', 'electra.encoder.layer.10.output.dense.bias', 'electra.encoder.layer.10.output.LayerNorm.weight', 'electra.encoder.layer.10.output.LayerNorm.bias', 'electra.encoder.layer.11.attention.self.query.weight', 'electra.encoder.layer.11.attention.self.query.bias', 'electra.encoder.layer.11.attention.self.key.weight', 'electra.encoder.layer.11.attention.self.key.bias', 'electra.encoder.layer.11.attention.self.value.weight', 'electra.encoder.layer.11.attention.self.value.bias', 'electra.encoder.layer.11.attention.output.dense.weight', 'electra.encoder.layer.11.attention.output.dense.bias', 'electra.encoder.layer.11.attention.output.LayerNorm.weight', 'electra.encoder.layer.11.attention.output.LayerNorm.bias', 'electra.encoder.layer.11.intermediate.dense.weight', 'electra.encoder.layer.11.intermediate.dense.bias', 'electra.encoder.layer.11.output.dense.weight', 'electra.encoder.layer.11.output.dense.bias', 'electra.encoder.layer.11.output.LayerNorm.weight', 'electra.encoder.layer.11.output.LayerNorm.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "slot_vocab = []\n",
    "for slot in slot_meta:\n",
    "    slot_vocab.append(\n",
    "        tokenizer.encode(slot.replace('-', ' '),\n",
    "                         add_special_tokens=False)\n",
    "    )\n",
    "    \n",
    "config = BertConfig.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "config.model_name_or_path = 'monologg/koelectra-base-v3-discriminator'\n",
    "config.n_gate = len(processor.gating2id)\n",
    "config.proj_dim = None\n",
    "model = TRADE(config, slot_vocab, slot_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ksvFxR-h04XI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = WOSDataset(train_features)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, batch_size=16, sampler=train_sampler, collate_fn=processor.collate_fn)\n",
    "\n",
    "dev_data = WOSDataset(dev_features)\n",
    "dev_sampler = SequentialSampler(dev_data)\n",
    "dev_loader = DataLoader(dev_data, batch_size=8, sampler=dev_sampler, collate_fn=processor.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVYxBaXE04XJ"
   },
   "source": [
    "# Optimizer & Scheduler 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NmDmrigU04XJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "t_total = len(train_loader) * n_epochs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0.1, num_training_steps=t_total\n",
    ")\n",
    "teacher_forcing = 0.5\n",
    "model.to(device)\n",
    "\n",
    "def masked_cross_entropy_for_value(logits, target, pad_idx=0):\n",
    "    mask = target.ne(pad_idx)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    log_probs_flat = torch.log(logits_flat)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / (mask.sum().float())\n",
    "    return loss\n",
    "\n",
    "loss_fnc_1 = masked_cross_entropy_for_value  # generation\n",
    "loss_fnc_2 = nn.CrossEntropyLoss()  # gating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0GK3Z8z04XJ"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrWnCWwC04XJ",
    "outputId": "194c265e-ab26-4993-a61e-1dca5d3274aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10] [0/2887] 10.232786\n",
      "[0/10] [100/2887] 2.391372\n",
      "[0/10] [200/2887] 1.803414\n",
      "[0/10] [300/2887] 1.570395\n",
      "[0/10] [400/2887] 1.419575\n",
      "[0/10] [500/2887] 0.981048\n",
      "[0/10] [600/2887] 0.895471\n",
      "[0/10] [700/2887] 1.308567\n",
      "[0/10] [800/2887] 1.227765\n",
      "[0/10] [900/2887] 1.107860\n",
      "[0/10] [1000/2887] 1.111655\n",
      "[0/10] [1100/2887] 0.995359\n",
      "[0/10] [1200/2887] 0.828354\n",
      "[0/10] [1300/2887] 0.669995\n",
      "[0/10] [1400/2887] 0.780039\n",
      "[0/10] [1500/2887] 0.754878\n",
      "[0/10] [1600/2887] 0.931506\n",
      "[0/10] [1700/2887] 0.520194\n",
      "[0/10] [1800/2887] 0.664952\n",
      "[0/10] [1900/2887] 0.746445\n",
      "[0/10] [2000/2887] 0.537805\n",
      "[0/10] [2100/2887] 0.599281\n",
      "[0/10] [2200/2887] 0.695270\n",
      "[0/10] [2300/2887] 0.460447\n",
      "[0/10] [2400/2887] 0.501357\n",
      "[0/10] [2500/2887] 0.589312\n",
      "[0/10] [2600/2887] 0.556324\n",
      "[0/10] [2700/2887] 0.607038\n",
      "[0/10] [2800/2887] 0.471369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [02:05<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.045032589373889, 'turn_slot_accuracy': 0.8910922378036775, 'turn_slot_f1': 0.49492477622631337}\n",
      "joint_goal_accuracy: 0.045032589373889\n",
      "turn_slot_accuracy: 0.8910922378036775\n",
      "turn_slot_f1: 0.49492477622631337\n",
      "[1/10] [0/2887] 0.509095\n",
      "[1/10] [100/2887] 0.463494\n",
      "[1/10] [200/2887] 0.734209\n",
      "[1/10] [300/2887] 0.539552\n",
      "[1/10] [400/2887] 0.406150\n",
      "[1/10] [500/2887] 0.361528\n",
      "[1/10] [600/2887] 0.412069\n",
      "[1/10] [700/2887] 0.435999\n",
      "[1/10] [800/2887] 0.392826\n",
      "[1/10] [900/2887] 0.388323\n",
      "[1/10] [1000/2887] 0.477644\n",
      "[1/10] [1100/2887] 0.417325\n",
      "[1/10] [1200/2887] 0.409482\n",
      "[1/10] [1300/2887] 0.376725\n",
      "[1/10] [1400/2887] 0.364903\n",
      "[1/10] [1500/2887] 0.367675\n",
      "[1/10] [1600/2887] 0.443778\n",
      "[1/10] [1700/2887] 0.366907\n",
      "[1/10] [1800/2887] 0.296813\n",
      "[1/10] [1900/2887] 0.309030\n",
      "[1/10] [2000/2887] 0.457582\n",
      "[1/10] [2100/2887] 0.328933\n",
      "[1/10] [2200/2887] 0.388200\n",
      "[1/10] [2300/2887] 0.306854\n",
      "[1/10] [2400/2887] 0.373714\n",
      "[1/10] [2500/2887] 0.294009\n",
      "[1/10] [2600/2887] 0.425231\n",
      "[1/10] [2700/2887] 0.280820\n",
      "[1/10] [2800/2887] 0.362776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [02:16<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.12364210942129172, 'turn_slot_accuracy': 0.9173919722606293, 'turn_slot_f1': 0.6589077155122807}\n",
      "joint_goal_accuracy: 0.12364210942129172\n",
      "turn_slot_accuracy: 0.9173919722606293\n",
      "turn_slot_f1: 0.6589077155122807\n",
      "[2/10] [0/2887] 0.440191\n",
      "[2/10] [100/2887] 0.556082\n",
      "[2/10] [200/2887] 0.404303\n",
      "[2/10] [300/2887] 0.287601\n",
      "[2/10] [400/2887] 0.346406\n",
      "[2/10] [500/2887] 0.267854\n",
      "[2/10] [600/2887] 0.202664\n",
      "[2/10] [700/2887] 0.196974\n",
      "[2/10] [800/2887] 0.252412\n",
      "[2/10] [900/2887] 0.417170\n",
      "[2/10] [1000/2887] 0.269452\n",
      "[2/10] [1100/2887] 0.251712\n",
      "[2/10] [1200/2887] 0.342814\n",
      "[2/10] [1300/2887] 0.335911\n",
      "[2/10] [1400/2887] 0.186859\n",
      "[2/10] [1500/2887] 0.366879\n",
      "[2/10] [1600/2887] 0.334059\n",
      "[2/10] [1700/2887] 0.295318\n",
      "[2/10] [1800/2887] 0.198391\n",
      "[2/10] [1900/2887] 0.207775\n",
      "[2/10] [2000/2887] 0.246142\n",
      "[2/10] [2100/2887] 0.262011\n",
      "[2/10] [2200/2887] 0.180518\n",
      "[2/10] [2300/2887] 0.208054\n",
      "[2/10] [2400/2887] 0.226337\n",
      "[2/10] [2500/2887] 0.233081\n",
      "[2/10] [2600/2887] 0.256373\n",
      "[2/10] [2700/2887] 0.243676\n",
      "[2/10] [2800/2887] 0.191924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [02:04<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.20047402725656724, 'turn_slot_accuracy': 0.9391313889437599, 'turn_slot_f1': 0.7425174080739521}\n",
      "joint_goal_accuracy: 0.20047402725656724\n",
      "turn_slot_accuracy: 0.9391313889437599\n",
      "turn_slot_f1: 0.7425174080739521\n",
      "[3/10] [0/2887] 0.123686\n",
      "[3/10] [100/2887] 0.291521\n",
      "[3/10] [200/2887] 0.165870\n",
      "[3/10] [300/2887] 0.253531\n",
      "[3/10] [400/2887] 0.231128\n",
      "[3/10] [500/2887] 0.206411\n",
      "[3/10] [600/2887] 0.211190\n",
      "[3/10] [700/2887] 0.186499\n",
      "[3/10] [800/2887] 0.196712\n",
      "[3/10] [900/2887] 0.263542\n",
      "[3/10] [1000/2887] 0.119411\n",
      "[3/10] [1100/2887] 0.224511\n",
      "[3/10] [1200/2887] 0.182824\n",
      "[3/10] [1300/2887] 0.179684\n",
      "[3/10] [1400/2887] 0.211874\n",
      "[3/10] [1500/2887] 0.186141\n",
      "[3/10] [1600/2887] 0.191578\n",
      "[3/10] [1700/2887] 0.196617\n",
      "[3/10] [1800/2887] 0.306638\n",
      "[3/10] [1900/2887] 0.278303\n",
      "[3/10] [2000/2887] 0.214814\n",
      "[3/10] [2100/2887] 0.114791\n",
      "[3/10] [2200/2887] 0.181626\n",
      "[3/10] [2300/2887] 0.212644\n",
      "[3/10] [2400/2887] 0.143318\n",
      "[3/10] [2500/2887] 0.187614\n",
      "[3/10] [2600/2887] 0.199058\n",
      "[3/10] [2700/2887] 0.168801\n",
      "[3/10] [2800/2887] 0.236233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [02:08<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.26170254789650405, 'turn_slot_accuracy': 0.9521495819343058, 'turn_slot_f1': 0.7944254055088275}\n",
      "joint_goal_accuracy: 0.26170254789650405\n",
      "turn_slot_accuracy: 0.9521495819343058\n",
      "turn_slot_f1: 0.7944254055088275\n",
      "[4/10] [0/2887] 0.130104\n",
      "[4/10] [100/2887] 0.175621\n",
      "[4/10] [200/2887] 0.158361\n",
      "[4/10] [300/2887] 0.163347\n",
      "[4/10] [400/2887] 0.155569\n",
      "[4/10] [500/2887] 0.085849\n",
      "[4/10] [600/2887] 0.183993\n",
      "[4/10] [700/2887] 0.155467\n",
      "[4/10] [800/2887] 0.224082\n",
      "[4/10] [900/2887] 0.215965\n",
      "[4/10] [1000/2887] 0.168282\n",
      "[4/10] [1100/2887] 0.095163\n",
      "[4/10] [1200/2887] 0.213468\n",
      "[4/10] [1300/2887] 0.124956\n",
      "[4/10] [1400/2887] 0.170860\n",
      "[4/10] [1500/2887] 0.184751\n",
      "[4/10] [1600/2887] 0.146808\n",
      "[4/10] [1700/2887] 0.152108\n",
      "[4/10] [1800/2887] 0.188485\n",
      "[4/10] [1900/2887] 0.139264\n",
      "[4/10] [2000/2887] 0.109258\n",
      "[4/10] [2100/2887] 0.172835\n",
      "[4/10] [2200/2887] 0.171065\n",
      "[4/10] [2300/2887] 0.171971\n",
      "[4/10] [2400/2887] 0.143322\n",
      "[4/10] [2500/2887] 0.164027\n",
      "[4/10] [2600/2887] 0.146006\n",
      "[4/10] [2700/2887] 0.174519\n",
      "[4/10] [2800/2887] 0.140944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [02:05<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.29685956942524194, 'turn_slot_accuracy': 0.9579739723923146, 'turn_slot_f1': 0.8210957543865344}\n",
      "joint_goal_accuracy: 0.29685956942524194\n",
      "turn_slot_accuracy: 0.9579739723923146\n",
      "turn_slot_f1: 0.8210957543865344\n",
      "[5/10] [0/2887] 0.143527\n",
      "[5/10] [100/2887] 0.101725\n",
      "[5/10] [200/2887] 0.177152\n",
      "[5/10] [300/2887] 0.116574\n",
      "[5/10] [400/2887] 0.097791\n",
      "[5/10] [500/2887] 0.109362\n",
      "[5/10] [600/2887] 0.099540\n",
      "[5/10] [700/2887] 0.164511\n",
      "[5/10] [800/2887] 0.150690\n",
      "[5/10] [900/2887] 0.097269\n",
      "[5/10] [1000/2887] 0.109693\n",
      "[5/10] [1100/2887] 0.123846\n",
      "[5/10] [1200/2887] 0.175840\n",
      "[5/10] [1300/2887] 0.140996\n",
      "[5/10] [1400/2887] 0.093736\n",
      "[5/10] [1500/2887] 0.124304\n",
      "[5/10] [1600/2887] 0.117026\n",
      "[5/10] [1700/2887] 0.123565\n",
      "[5/10] [1800/2887] 0.120061\n",
      "[5/10] [1900/2887] 0.098521\n",
      "[5/10] [2000/2887] 0.090652\n",
      "[5/10] [2100/2887] 0.138054\n",
      "[5/10] [2200/2887] 0.130291\n",
      "[5/10] [2300/2887] 0.087895\n",
      "[5/10] [2400/2887] 0.155123\n",
      "[5/10] [2500/2887] 0.098515\n",
      "[5/10] [2600/2887] 0.117193\n",
      "[5/10] [2700/2887] 0.130060\n",
      "[5/10] [2800/2887] 0.126281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [02:15<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.30890776219632626, 'turn_slot_accuracy': 0.9608181359317172, 'turn_slot_f1': 0.8302197777906122}\n",
      "joint_goal_accuracy: 0.30890776219632626\n",
      "turn_slot_accuracy: 0.9608181359317172\n",
      "turn_slot_f1: 0.8302197777906122\n",
      "[6/10] [0/2887] 0.095042\n",
      "[6/10] [100/2887] 0.087394\n",
      "[6/10] [200/2887] 0.169536\n",
      "[6/10] [300/2887] 0.119359\n",
      "[6/10] [400/2887] 0.088280\n",
      "[6/10] [500/2887] 0.109889\n",
      "[6/10] [600/2887] 0.083691\n",
      "[6/10] [700/2887] 0.146076\n",
      "[6/10] [800/2887] 0.079794\n",
      "[6/10] [900/2887] 0.108289\n",
      "[6/10] [1000/2887] 0.094878\n",
      "[6/10] [1100/2887] 0.077355\n",
      "[6/10] [1200/2887] 0.075294\n",
      "[6/10] [1300/2887] 0.104162\n",
      "[6/10] [1400/2887] 0.140677\n",
      "[6/10] [1500/2887] 0.101692\n",
      "[6/10] [1600/2887] 0.112577\n",
      "[6/10] [1700/2887] 0.090914\n",
      "[6/10] [1800/2887] 0.081956\n",
      "[6/10] [1900/2887] 0.150973\n",
      "[6/10] [2000/2887] 0.110440\n",
      "[6/10] [2100/2887] 0.070172\n",
      "[6/10] [2200/2887] 0.132107\n",
      "[6/10] [2300/2887] 0.101544\n",
      "[6/10] [2400/2887] 0.117808\n",
      "[6/10] [2500/2887] 0.093373\n",
      "[6/10] [2600/2887] 0.087126\n",
      "[6/10] [2700/2887] 0.107932\n",
      "[6/10] [2800/2887] 0.102320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [02:15<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.32549871617618015, 'turn_slot_accuracy': 0.963508679526864, 'turn_slot_f1': 0.8393643467229069}\n",
      "joint_goal_accuracy: 0.32549871617618015\n",
      "turn_slot_accuracy: 0.963508679526864\n",
      "turn_slot_f1: 0.8393643467229069\n",
      "[7/10] [0/2887] 0.116394\n",
      "[7/10] [100/2887] 0.076097\n",
      "[7/10] [200/2887] 0.106540\n",
      "[7/10] [300/2887] 0.087654\n",
      "[7/10] [400/2887] 0.095442\n",
      "[7/10] [500/2887] 0.105933\n",
      "[7/10] [600/2887] 0.064033\n",
      "[7/10] [700/2887] 0.084851\n",
      "[7/10] [800/2887] 0.098793\n",
      "[7/10] [900/2887] 0.104844\n",
      "[7/10] [1000/2887] 0.092339\n",
      "[7/10] [1100/2887] 0.083415\n",
      "[7/10] [1200/2887] 0.158005\n",
      "[7/10] [1300/2887] 0.105494\n",
      "[7/10] [1400/2887] 0.120625\n",
      "[7/10] [1500/2887] 0.095504\n",
      "[7/10] [1600/2887] 0.075909\n",
      "[7/10] [1700/2887] 0.141055\n",
      "[7/10] [1800/2887] 0.052702\n",
      "[7/10] [1900/2887] 0.134036\n",
      "[7/10] [2000/2887] 0.081664\n",
      "[7/10] [2100/2887] 0.104094\n",
      "[7/10] [2200/2887] 0.064004\n",
      "[7/10] [2300/2887] 0.099604\n",
      "[7/10] [2400/2887] 0.071862\n",
      "[7/10] [2500/2887] 0.121801\n",
      "[7/10] [2600/2887] 0.118193\n",
      "[7/10] [2700/2887] 0.115431\n",
      "[7/10] [2800/2887] 0.045114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [02:04<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.3389294884455856, 'turn_slot_accuracy': 0.9650668246757633, 'turn_slot_f1': 0.8476121021069808}\n",
      "joint_goal_accuracy: 0.3389294884455856\n",
      "turn_slot_accuracy: 0.9650668246757633\n",
      "turn_slot_f1: 0.8476121021069808\n",
      "[8/10] [0/2887] 0.087889\n",
      "[8/10] [100/2887] 0.101556\n",
      "[8/10] [200/2887] 0.075228\n",
      "[8/10] [300/2887] 0.098476\n"
     ]
    }
   ],
   "source": [
    " for epoch in range(n_epochs):\n",
    "    batch_loss = []\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids, segment_ids, input_masks, gating_ids, target_ids, guids = [b.to(device) if not isinstance(b, list) else b for b in batch]\n",
    "        if teacher_forcing > 0.0 and random.random() < teacher_forcing:\n",
    "            tf = target_ids\n",
    "        else:\n",
    "            tf = None\n",
    "\n",
    "        all_point_outputs, all_gate_outputs = model(input_ids, segment_ids, input_masks, target_ids.size(-1))  # gt - length (generation)\n",
    "        loss_1 = loss_fnc_1(all_point_outputs.contiguous(), target_ids.contiguous().view(-1))\n",
    "        loss_2 = loss_fnc_2(all_gate_outputs.contiguous().view(-1, 5), gating_ids.contiguous().view(-1))\n",
    "        loss = loss_1 + loss_2\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step % 100 == 0:\n",
    "            print('[%d/%d] [%d/%d] %f' % (epoch, n_epochs, step, len(train_loader), loss.item()))\n",
    "                \n",
    "    predictions = inference(model, dev_loader, processor, device)\n",
    "    eval_result = _evaluation(predictions, dev_labels, slot_meta)\n",
    "    for k, v in eval_result.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR4EEzbz04XK"
   },
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOJqYcpP04XK",
    "outputId": "8947b1d7-fcdc-4c4b-8536-a8ee7a057123"
   },
   "outputs": [],
   "source": [
    "eval_data = json.load(open(f\"/opt/ml/input/data/eval_dataset/eval_dials.json\", \"r\"))\n",
    "\n",
    "eval_examples = get_examples_from_dialogues(\n",
    "    eval_data, user_first=False, dialogue_level=False\n",
    ")\n",
    "\n",
    "# Extracting Featrues\n",
    "eval_features = processor.convert_examples_to_features(eval_examples)\n",
    "eval_data = WOSDataset(eval_features)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_loader = DataLoader(\n",
    "    eval_data,\n",
    "    batch_size=8,\n",
    "    sampler=eval_sampler,\n",
    "    collate_fn=processor.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLU16_Ur04XL",
    "outputId": "c30e4ae5-5212-41f8-f171-0ff5e56b02fb"
   },
   "outputs": [],
   "source": [
    "predictions = inference(model, eval_loader, processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0uR9J8e04XL"
   },
   "outputs": [],
   "source": [
    "json.dump(predictions, open('predictions.csv', 'w'), indent=2, ensure_ascii=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhrO4LRQ04XL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TRADE_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
